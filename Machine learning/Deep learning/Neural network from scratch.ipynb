{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-29T20:06:08.731826Z",
     "start_time": "2025-08-29T20:06:06.288110Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def one_hot(y, n_classes=None):\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    if n_classes is None:\n",
    "        n_classes = np.max(y) + 1\n",
    "    enc = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "    enc.fit(np.arange(n_classes).reshape(-1, 1))\n",
    "    return enc.transform(y)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# ---------------------------\n",
    "# Weight initialization\n",
    "# ---------------------------\n",
    "def init_weights(shape, method='xavier'):\n",
    "    fan_in, fan_out = shape[0], shape[1]\n",
    "    if method == 'xavier':\n",
    "        limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        return np.random.uniform(-limit, limit, size=shape)\n",
    "    elif method == 'he':\n",
    "        std = np.sqrt(2.0 / fan_in)\n",
    "        return np.random.randn(*shape) * std\n",
    "    else:\n",
    "        return np.random.randn(*shape) * 0.01\n",
    "\n",
    "# ---------------------------\n",
    "# Layers\n",
    "# ---------------------------\n",
    "class Layer:\n",
    "    def forward(self, X, training=True):\n",
    "        raise NotImplementedError\n",
    "    def backward(self, grad):\n",
    "        raise NotImplementedError\n",
    "    def params(self):\n",
    "        return []\n",
    "    def grads(self):\n",
    "        return []\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, in_dim, out_dim, weight_init='xavier', bias_init=0.0, l2=0.0):\n",
    "        self.W = init_weights((in_dim, out_dim), method=weight_init)\n",
    "        self.b = np.zeros((1, out_dim)) + bias_init\n",
    "        self._cache = None\n",
    "        self.l2 = l2  # weight decay multiplier\n",
    "\n",
    "        # gradients\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self._cache = X\n",
    "        return X.dot(self.W) + self.b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        X = self._cache  # (N, D)\n",
    "        N = X.shape[0]\n",
    "        self.dW = (X.T.dot(grad)) / N + self.l2 * self.W\n",
    "        self.db = np.sum(grad, axis=0, keepdims=True) / N\n",
    "        dX = grad.dot(self.W.T)\n",
    "        return dX\n",
    "\n",
    "    def params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def grads(self):\n",
    "        return [self.dW, self.db]\n",
    "\n",
    "class BatchNorm(Layer):\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.9):\n",
    "        self.gamma = np.ones((1, dim))\n",
    "        self.beta = np.zeros((1, dim))\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = np.zeros((1, dim))\n",
    "        self.running_var = np.ones((1, dim))\n",
    "        self._cache = None\n",
    "        self.dgamma = np.zeros_like(self.gamma)\n",
    "        self.dbeta = np.zeros_like(self.beta)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            mu = np.mean(X, axis=0, keepdims=True)\n",
    "            var = np.var(X, axis=0, keepdims=True)\n",
    "            x_hat = (X - mu) / np.sqrt(var + self.eps)\n",
    "            out = self.gamma * x_hat + self.beta\n",
    "            self._cache = (X, x_hat, mu, var)\n",
    "            # update running stats\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "            return out\n",
    "        else:\n",
    "            x_hat = (X - self.running_mean) / np.sqrt(self.running_var + self.eps)\n",
    "            return self.gamma * x_hat + self.beta\n",
    "\n",
    "    def backward(self, dout):\n",
    "        X, x_hat, mu, var = self._cache\n",
    "        N = X.shape[0]\n",
    "        std_inv = 1.0 / np.sqrt(var + self.eps)\n",
    "\n",
    "        self.dgamma = np.sum(dout * x_hat, axis=0, keepdims=True)\n",
    "        self.dbeta = np.sum(dout, axis=0, keepdims=True)\n",
    "\n",
    "        dx_hat = dout * self.gamma\n",
    "        dvar = np.sum(dx_hat * (X - mu) * -0.5 * std_inv**3, axis=0, keepdims=True)\n",
    "        dmu = np.sum(dx_hat * -std_inv, axis=0, keepdims=True) + dvar * np.mean(-2.0 * (X - mu), axis=0, keepdims=True)\n",
    "\n",
    "        dx = dx_hat * std_inv + dvar * 2.0 * (X - mu) / N + dmu / N\n",
    "        return dx\n",
    "\n",
    "    def params(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "    def grads(self):\n",
    "        return [self.dgamma, self.dbeta]\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, p=0.5):\n",
    "        assert 0 <= p < 1\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if not training or self.p == 0:\n",
    "            return X\n",
    "        self.mask = (np.random.rand(*X.shape) > self.p) / (1.0 - self.p)\n",
    "        return X * self.mask\n",
    "\n",
    "    def backward(self, grad):\n",
    "        if self.mask is None:\n",
    "            return grad\n",
    "        return grad * self.mask\n",
    "\n",
    "# ---------------------------\n",
    "# Activations\n",
    "# ---------------------------\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        self._cache = None\n",
    "    def forward(self, X, training=True):\n",
    "        self._cache = X\n",
    "        return np.maximum(0, X)\n",
    "    def backward(self, grad):\n",
    "        X = self._cache\n",
    "        d = grad.copy()\n",
    "        d[X <= 0] = 0\n",
    "        return d\n",
    "\n",
    "class LeakyReLU(Layer):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self._cache = None\n",
    "    def forward(self, X, training=True):\n",
    "        self._cache = X\n",
    "        return np.where(X > 0, X, self.alpha * X)\n",
    "    def backward(self, grad):\n",
    "        X = self._cache\n",
    "        d = grad.copy()\n",
    "        d[X <= 0] *= self.alpha\n",
    "        return d\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        self._cache = None\n",
    "    def forward(self, X, training=True):\n",
    "        s = 1.0 / (1.0 + np.exp(-X))\n",
    "        self._cache = s\n",
    "        return s\n",
    "    def backward(self, grad):\n",
    "        s = self._cache\n",
    "        return grad * s * (1 - s)\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        self._cache = None\n",
    "    def forward(self, X, training=True):\n",
    "        t = np.tanh(X)\n",
    "        self._cache = t\n",
    "        return t\n",
    "    def backward(self, grad):\n",
    "        t = self._cache\n",
    "        return grad * (1 - t**2)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    \"\"\"\n",
    "    Softmax layer outputs probabilities row-wise.\n",
    "    Usually combined with cross-entropy loss for numerical stability.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._cache = None\n",
    "    def forward(self, X, training=True):\n",
    "        # numerically stable softmax\n",
    "        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self._cache = probs\n",
    "        return probs\n",
    "    def backward(self, grad):\n",
    "        # grad expected to be dL/dY where Y is softmax output.\n",
    "        # J = diag(p) - p p^T ; implementing full Jacobian-vector product per-sample\n",
    "        p = self._cache\n",
    "        dX = np.empty_like(grad)\n",
    "        for i in range(p.shape[0]):\n",
    "            pi = p[i].reshape(-1, 1)\n",
    "            Ji = np.diagflat(pi) - pi.dot(pi.T)\n",
    "            dX[i] = grad[i].dot(Ji)\n",
    "        return dX\n",
    "\n",
    "# ---------------------------\n",
    "# Losses\n",
    "# ---------------------------\n",
    "class CrossEntropyLoss:\n",
    "    \"\"\"\n",
    "    Combines softmax + cross-entropy in a numerically stable way.\n",
    "    Assumes logits input (before softmax).\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(logits, y_onehot):\n",
    "        # logits: (N, C), y_onehot: (N, C)\n",
    "        # stable log-softmax\n",
    "        shifted = logits - np.max(logits, axis=1, keepdims=True)\n",
    "        log_probs = shifted - np.log(np.sum(np.exp(shifted), axis=1, keepdims=True))\n",
    "        loss = -np.sum(y_onehot * log_probs) / logits.shape[0]\n",
    "        probs = np.exp(log_probs)\n",
    "        return loss, probs  # also returns probs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(probs, y_onehot):\n",
    "        # dL/dlogits = (probs - y) / N\n",
    "        N = probs.shape[0]\n",
    "        return (probs - y_onehot) / N\n",
    "\n",
    "class MSELoss:\n",
    "    @staticmethod\n",
    "    def forward(pred, target):\n",
    "        loss = np.mean((pred - target) ** 2)\n",
    "        return loss\n",
    "    @staticmethod\n",
    "    def backward(pred, target):\n",
    "        N = pred.shape[0]\n",
    "        return 2 * (pred - target) / N\n",
    "\n",
    "# ---------------------------\n",
    "# Optimizers\n",
    "# ---------------------------\n",
    "class SGD_Momentum:\n",
    "    def __init__(self, params: List[np.ndarray], lr=1e-3, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = [np.zeros_like(p) for p in params]\n",
    "        self.params = params\n",
    "\n",
    "    def step(self, grads: List[np.ndarray]):\n",
    "        for i, (p, g) in enumerate(zip(self.params, grads)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * g\n",
    "            p += self.v[i]  # in-place update\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, params: List[np.ndarray], lr=1e-3, b1=0.9, b2=0.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.eps = eps\n",
    "        self.m = [np.zeros_like(p) for p in params]\n",
    "        self.v = [np.zeros_like(p) for p in params]\n",
    "        self.t = 0\n",
    "        self.params = params\n",
    "\n",
    "    def step(self, grads: List[np.ndarray]):\n",
    "        self.t += 1\n",
    "        for i, (p, g) in enumerate(zip(self.params, grads)):\n",
    "            self.m[i] = self.b1 * self.m[i] + (1 - self.b1) * g\n",
    "            self.v[i] = self.b2 * self.v[i] + (1 - self.b2) * (g * g)\n",
    "            m_hat = self.m[i] / (1 - self.b1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.b2 ** self.t)\n",
    "            p -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "# ---------------------------\n",
    "# Model\n",
    "# ---------------------------\n",
    "class NeuralNet:\n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        out = X\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out, training=training)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad):\n",
    "        dout = grad\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def params_and_grads(self) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        ps, gs = [], []\n",
    "        for layer in self.layers:\n",
    "            for p, g in zip(layer.params(), layer.grads()):\n",
    "                ps.append(p)\n",
    "                gs.append(g)\n",
    "        return ps, gs\n",
    "\n",
    "    def predict(self, X):\n",
    "        logits = self.forward(X, training=False)\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        return preds\n",
    "\n",
    "    def save(self, path):\n",
    "        # Save parameters\n",
    "        params = [p for layer in self.layers for p in layer.params()]\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            layer_params = layer.params()\n",
    "            for j in range(len(layer_params)):\n",
    "                layer_params[j][...] = params[i]\n",
    "                i += 1\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop\n",
    "# ---------------------------\n",
    "def train(model: NeuralNet, X_train, y_train_onehot, X_val, y_val, epochs=100, batch_size=64,\n",
    "          optimizer_type='adam', lr=1e-3, weight_decay=0.0, lr_decay_step=50, lr_decay_gamma=0.5,\n",
    "          verbose=1):\n",
    "    # collect parameters and grads\n",
    "    params, grads = model.params_and_grads()\n",
    "    if optimizer_type.lower() == 'adam':\n",
    "        opt = Adam(params, lr=lr)\n",
    "    else:\n",
    "        opt = SGD_Momentum(params, lr=lr, momentum=0.9)\n",
    "\n",
    "    n = X_train.shape[0]\n",
    "    history = {'train_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # shuffle\n",
    "        perm = np.random.permutation(n)\n",
    "        X_shuf = X_train[perm]\n",
    "        y_shuf = y_train_onehot[perm]\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, n, batch_size):\n",
    "            xb = X_shuf[i:i+batch_size]\n",
    "            yb = y_shuf[i:i+batch_size]\n",
    "\n",
    "            logits = model.forward(xb, training=True)\n",
    "            loss, probs = CrossEntropyLoss.forward(logits, yb)\n",
    "            epoch_loss += loss * xb.shape[0]\n",
    "\n",
    "            # backward\n",
    "            dlogits = CrossEntropyLoss.backward(probs, yb)\n",
    "            model.backward(dlogits)\n",
    "\n",
    "            # collect grads and params (they reference actual layer arrays)\n",
    "            _, grads = model.params_and_grads()\n",
    "\n",
    "            # step optimizer\n",
    "            opt.step(grads)\n",
    "\n",
    "        epoch_loss /= n\n",
    "\n",
    "        # LR scheduler step decay\n",
    "        if epoch % lr_decay_step == 0:\n",
    "            new_lr = opt.lr * lr_decay_gamma\n",
    "            opt.set_lr(new_lr)\n",
    "\n",
    "        # validation accuracy\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_acc = accuracy(y_val, val_preds)\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == 1 or epoch == epochs):\n",
    "            print(f\"Epoch {epoch}/{epochs} - loss: {epoch_loss:.4f} - val_acc: {val_acc:.4f} - lr: {opt.lr:.6f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a synthetic multiclass dataset\n",
    "    X, y = make_classification(n_samples=1500, n_features=20, n_informative=15,\n",
    "                               n_redundant=2, n_classes=3, random_state=42)\n",
    "    X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-9)  # simple normalization\n",
    "\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=1)\n",
    "\n",
    "    y_train_oh = one_hot(y_train, n_classes=3)\n",
    "    y_val_oh = one_hot(y_val, n_classes=3)\n",
    "\n",
    "    # Build model: Input(20) -> Linear(128) -> BatchNorm -> ReLU -> Dropout -> Linear(64) -> ReLU -> Linear(3)\n",
    "    layers = [\n",
    "        Linear(20, 128, weight_init='he', l2=1e-4),\n",
    "        BatchNorm(128),\n",
    "        ReLU(),\n",
    "        Dropout(p=0.2),\n",
    "        Linear(128, 64, weight_init='he', l2=1e-4),\n",
    "        ReLU(),\n",
    "        Linear(64, 3, weight_init='xavier')\n",
    "    ]\n",
    "    net = NeuralNet(layers)\n",
    "\n",
    "    # Train\n",
    "    history = train(net, X_train, y_train_oh, X_val, y_val,\n",
    "                    epochs=80, batch_size=64, optimizer_type='adam', lr=1e-3,\n",
    "                    lr_decay_step=30, lr_decay_gamma=0.5, verbose=1)\n",
    "\n",
    "    # Final test accuracy\n",
    "    preds_test = net.predict(X_test)\n",
    "    print(\"Test accuracy:\", accuracy(y_test, preds_test))\n",
    "\n",
    "    # Save model\n",
    "    net.save(\"model_params.pkl\")\n",
    "    print(\"Saved model to model_params.pkl\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 - loss: 1.0692 - val_acc: 0.6222 - lr: 0.001000\n",
      "Epoch 8/80 - loss: 0.5520 - val_acc: 0.7867 - lr: 0.001000\n",
      "Epoch 16/80 - loss: 0.3793 - val_acc: 0.8400 - lr: 0.001000\n",
      "Epoch 24/80 - loss: 0.3033 - val_acc: 0.8667 - lr: 0.001000\n",
      "Epoch 32/80 - loss: 0.2488 - val_acc: 0.9022 - lr: 0.000500\n",
      "Epoch 40/80 - loss: 0.2123 - val_acc: 0.8978 - lr: 0.000500\n",
      "Epoch 48/80 - loss: 0.1946 - val_acc: 0.9067 - lr: 0.000500\n",
      "Epoch 56/80 - loss: 0.1851 - val_acc: 0.9067 - lr: 0.000500\n",
      "Epoch 64/80 - loss: 0.1790 - val_acc: 0.9067 - lr: 0.000250\n",
      "Epoch 72/80 - loss: 0.1709 - val_acc: 0.9156 - lr: 0.000250\n",
      "Epoch 80/80 - loss: 0.1652 - val_acc: 0.9289 - lr: 0.000250\n",
      "Test accuracy: 0.9111111111111111\n",
      "Saved model to model_params.pkl\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
