{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ensemble Learning: Bagging, Boosting, and Stacking\n",
    "\n",
    "## 1. Bagging (Bootstrap Aggregating)\n",
    "\n",
    "**Definition:**\n",
    "Bagging is an **ensemble learning technique** aimed at reducing the **variance** of a model by training multiple base learners on different **bootstrap samples** (sampling with replacement) of the dataset, and then aggregating their predictions.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "Given a dataset:\n",
    "$$\n",
    "D = \\{(x_i, y_i)\\}_{i=1}^n\n",
    "$$\n",
    "\n",
    "We create \\( B \\) bootstrap samples:\n",
    "$$\n",
    "D_1, D_2, \\dots, D_B\n",
    "$$\n",
    "\n",
    "For each sample, we train a base learner \\( h_b(x) \\). The final prediction is:\n",
    "\n",
    "- **Regression:**\n",
    "$$\n",
    "\\hat{f}_{bag}(x) = \\frac{1}{B} \\sum_{b=1}^{B} h_b(x)\n",
    "$$\n",
    "\n",
    "- **Classification:**\n",
    "$$\n",
    "\\hat{f}_{bag}(x) = \\text{majority\\_vote}\\{h_1(x), h_2(x), \\dots, h_B(x)\\}\n",
    "$$\n",
    "\n",
    "**Key Insight:**\n",
    "Bagging works best for **high-variance** models (like Decision Trees).\n",
    "The most popular example is **Random Forest**, which adds an additional random feature selection step.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Boosting\n",
    "\n",
    "**Definition:**\n",
    "Boosting is an **iterative (sequential) ensemble technique** where each new weak learner focuses on the **errors (residuals)** of the previous learners. The goal is to reduce **bias and variance** simultaneously.\n",
    "\n",
    "**AdaBoost Intuition:**\n",
    "Start with equal weights for all samples:\n",
    "$$\n",
    "w_i = \\frac{1}{n}\n",
    "$$\n",
    "\n",
    "At iteration \\( t \\), train a weak learner \\( h_t(x) \\) minimizing the **weighted error**:\n",
    "$$\n",
    "\\epsilon_t = \\sum_{i=1}^n w_i \\mathbf{1}(h_t(x_i) \\neq y_i)\n",
    "$$\n",
    "\n",
    "Compute the weight of the learner:\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2} \\ln\\frac{1-\\epsilon_t}{\\epsilon_t}\n",
    "$$\n",
    "\n",
    "Update sample weights:\n",
    "$$\n",
    "w_i \\leftarrow w_i \\cdot e^{-\\alpha_t y_i h_t(x_i)}\n",
    "$$\n",
    "Normalize \\( w_i \\).\n",
    "\n",
    "Final classifier:\n",
    "$$\n",
    "H(x) = \\text{sign}\\Big( \\sum_{t=1}^T \\alpha_t h_t(x) \\Big)\n",
    "$$\n",
    "\n",
    "**Gradient Boosting:**\n",
    "Instead of reweighting samples explicitly, models are fit to the **negative gradients of the loss function**. This connects boosting with gradient descent optimization.\n",
    "\n",
    "**Key Insight:**\n",
    "Boosting works best for **reducing bias**, but can overfit if not regularized (shrinkage, early stopping).\n",
    "\n",
    "Popular algorithms: **AdaBoost, Gradient Boosting, XGBoost, LightGBM.**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Stacking (Stacked Generalization)\n",
    "\n",
    "**Definition:**\n",
    "Stacking is a **meta-learning technique** where predictions from multiple diverse models are combined using another model called the **meta-learner**.\n",
    "\n",
    "**Process:**\n",
    "1. Train base learners \\( h_1, h_2, \\dots, h_k \\) on the training data.\n",
    "2. Generate a meta-feature matrix \\( Z \\):\n",
    "$$\n",
    "Z = [h_1(x), h_2(x), \\dots, h_k(x)]\n",
    "$$\n",
    "3. Train a meta-learner \\( g \\) on \\( Z \\) to produce the final prediction:\n",
    "$$\n",
    "\\hat{y} = g(Z)\n",
    "$$\n",
    "\n",
    "**Key Insight:**\n",
    "Stacking leverages **model diversity** and allows the meta-learner to learn the optimal combination of models.\n",
    "\n",
    "It requires careful **cross-validation** to avoid overfitting and data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Key Differences\n",
    "\n",
    "| Technique | Training Type | Goal | Example Models |\n",
    "|-----------|---------------|------|----------------|\n",
    "| Bagging   | Parallel (independent learners) | Reduce variance | Random Forest |\n",
    "| Boosting  | Sequential (each learner fixes previous errors) | Reduce bias & variance | AdaBoost, XGBoost |\n",
    "| Stacking  | Parallel + Meta-model | Learn optimal model combination | Kaggle meta-models |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Theoretical Insights\n",
    "\n",
    "- **Bias-Variance Trade-off:**\n",
    "  - Bagging: Primarily reduces variance.\n",
    "  - Boosting: Reduces bias and variance.\n",
    "  - Stacking: Exploits strengths of heterogeneous models.\n",
    "\n",
    "- **Generalization:**\n",
    "  Stacking often achieves the best performance when the base models are diverse and the meta-learner is chosen carefully.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "- Breiman, L. (1996). *Bagging Predictors*. Machine Learning.\n",
    "- Freund, Y., & Schapire, R. (1997). *A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting*.\n",
    "- Wolpert, D. (1992). *Stacked Generalization*.\n"
   ],
   "id": "6c423da3ed93c7bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
