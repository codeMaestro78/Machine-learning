{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Decision Tree — From Scratch\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Overview & Intuition](#overview--intuition)\n",
    "2. [High-level Algorithm](#high-level-algorithm)\n",
    "3. [Impurity Measures (Math & Intuition)](#impurity-measures-math--intuition)\n",
    "\n",
    "   * [Entropy](#entropy)\n",
    "   * [Gini Impurity](#gini-impurity)\n",
    "   * [Information Gain and Gini Decrease](#information-gain-and-gini-decrease)\n",
    "4. [Splitting Strategies](#splitting-strategies)\n",
    "\n",
    "   * [Continuous Features](#continuous-features)\n",
    "   * [Categorical Features](#categorical-features)\n",
    "5. [Stopping Criteria & Regularization](#stopping-criteria--regularization)\n",
    "6. [Cost-Complexity (Weakest-Link) Pruning — Sketch](#cost-complexity-weakest-link-pruning---sketch)\n",
    "7. [Complexity & Bias–Variance](#complexity--biasvariance)\n",
    "\n",
    "8. [Worked Numerical Example (Gini & Split)](#worked-numerical-example-gini--split)\n",
    "9. [Extensions & Production Notes](#extensions--production-notes)\n",
    "10. [References & Further Reading](#references--further-reading)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview & Intuition\n",
    "\n",
    "A **Decision Tree** is a supervised learning model that recursively partitions the feature space into regions that are increasingly homogeneous with respect to the target variable. Each internal node applies a test (e.g., `feature_j <= threshold`) and each leaf returns a prediction (a class label or a value).\n",
    "\n",
    "Why it works: at each split we aim to reduce label uncertainty (impurity). The goal is to find splits that create child nodes much purer than the parent.\n",
    "\n",
    "Key advantages: interpretability, non-linear decision boundaries, handles mixed types (with preprocessing). Drawbacks: high variance and tendency to overfit without regularization or pruning.\n",
    "\n",
    "---\n",
    "\n",
    "## High-level Algorithm\n",
    "\n",
    "1. Start with all training samples at the root node.\n",
    "2. If stopping conditions are met (pure node, max depth, etc.), create a leaf predicting the dominant class.\n",
    "3. Otherwise, find the best feature and threshold that **maximizes impurity reduction**.\n",
    "4. Split the samples and recurse on left and right children.\n",
    "5. Optionally prune the resulting tree using validation-based or cost-complexity pruning.\n",
    "\n",
    "This greedy recursive algorithm builds a tree top-down (also called recursive binary splitting for CART).\n",
    "\n",
    "---\n",
    "\n",
    "## Impurity Measures (Math & Intuition)\n",
    "\n",
    "We measure how \"mixed\" the labels are inside a node. Two popular choices are **Entropy** and **Gini impurity**.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "For a node containing samples from classes $1\\ldots k$ with class probabilities $p_1,\\dots,p_k$ (estimated as relative frequencies), the entropy is:\n",
    "\n",
    "$$\n",
    "H = -\\sum_{i=1}^k p_i \\log_2 p_i.\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "\n",
    "* $H=0$ when the node is pure (one class probability = 1).\n",
    "* Maximal when classes are uniformly distributed: $H_{max}=\\log_2 k$.\n",
    "\n",
    "**Information Gain (IG)** for a split that produces left and right children with entropies $H_L, H_R$ and sample-weight fractions $w_L, w_R$ is:\n",
    "\n",
    "$$\n",
    "\\text{IG} = H_{parent} - (w_L H_L + w_R H_R).\n",
    "$$\n",
    "\n",
    "We choose the split with the largest IG.\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "Gini impurity is defined as:\n",
    "\n",
    "$$\n",
    "G = 1 - \\sum_{i=1}^k p_i^2 = \\sum_{i\\neq j} p_i p_j.\n",
    "$$\n",
    "\n",
    "Interpretation: probability that two samples drawn at random (with replacement) from the node have different labels. Gini range: 0 (pure) to $1-1/k$.\n",
    "\n",
    "**Gini decrease** (gain) for a split:\n",
    "\n",
    "$$\n",
    "\\Delta G = G_{parent} - (w_L G_L + w_R G_R).\n",
    "$$\n",
    "\n",
    "CART (Classification And Regression Trees) typically uses Gini; ID3/C4.5 use entropy.\n",
    "\n",
    "### Practical note\n",
    "\n",
    "Entropy and Gini often select similar splits; Gini is slightly cheaper to compute and commonly used.\n",
    "\n",
    "---\n",
    "\n",
    "## Splitting Strategies\n",
    "\n",
    "### Continuous Features\n",
    "\n",
    "For a numeric feature $x$, candidate thresholds are chosen between sorted unique values. Let sorted distinct values be $v_1< v_2<\\dots< v_m$. Common candidate thresholds are midpoints:\n",
    "\n",
    "$$\n",
    "\\tau_i = \\frac{v_i + v_{i+1}}{2},\\quad i=1\\dots m-1.\n",
    "$$\n",
    "\n",
    "Test splits $x \\le \\tau_i$ vs $x>\\tau_i$ and compute impurity decrease. For efficiency, sort $x$ once per node and scan to compute impurities in linear time using running counts.\n",
    "\n",
    "### Categorical Features\n",
    "\n",
    "If a categorical feature has $m$ categories, an exhaustive split tests all non-trivial subsets (up to $2^{m-1}-1$ splits) — expensive when $m$ is large. Practical alternatives:\n",
    "\n",
    "* One-hot encode and treat as binary features.\n",
    "* Order categories by target mean and consider splits on that ordered list (common trick).\n",
    "\n",
    "---\n",
    "\n",
    "## Stopping Criteria & Regularization\n",
    "\n",
    "To avoid overfitting:\n",
    "\n",
    "* `max_depth`: maximum allowed depth of the tree.\n",
    "* `min_samples_split`: minimum samples required to try splitting a node.\n",
    "* `min_samples_leaf`: minimum samples required in each leaf after a split.\n",
    "* `max_features`: consider only a subset of features at each split (used in Random Forests).\n",
    "* `ccp_alpha`: complexity parameter for cost-complexity pruning (scikit-learn name).\n",
    "\n",
    "Pre-pruning stops tree growth early; post-pruning builds tree fully and prunes back.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost-Complexity (Weakest-Link) Pruning — Sketch\n",
    "\n",
    "Cost-complexity pruning chooses a subtree that minimizes:\n",
    "\n",
    "$$\n",
    "R_\\alpha(T) = R(T) + \\alpha |T|\n",
    "$$\n",
    "\n",
    "where $R(T)$ is the training error (or impurity-based risk) of tree $T$ and $|T|$ is number of leaves. Increasing $\\alpha$ prefers smaller trees. The weakest-link algorithm finds the sequence of optimal subtrees for increasing $\\alpha$ and picks the best via validation.\n",
    "\n",
    "A full implementation requires storing node impurities and leaf counts, and iteratively collapsing the internal node whose collapse yields smallest increase in the risk per pruned leaf (the \"weakest link\").\n",
    "\n",
    "---\n",
    "\n",
    "## Complexity & Bias–Variance\n",
    "\n",
    "* Building a tree naively: per split, evaluating all features and all thresholds costs roughly $O(n f \\log n)$ because sorting is $O(n \\log n)$. With pre-sorting and clever updates it's possible to get amortized $O(n f)$ per level.\n",
    "* Trees are low-bias (can fit complex patterns) but high-variance. Ensembles (Random Forests, Gradient Boosting) are used to reduce variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Full From-Scratch Implementation (Python)\n",
    "\n",
    "### Features of this implementation\n",
    "\n",
    "* CART-style binary splits using **Gini impurity**.\n",
    "* Handles numeric features.\n",
    "* Stores training indices at each node (so we can perform validation-based post-pruning).\n",
    "* Mutable `Node` class allowing in-place pruning.\n",
    "* `fit`, `predict`, `score`, and `prune` methods.\n",
    "\n",
    "> Note: This implementation is educational and prioritizes clarity over extreme performance. For production use prefer optimized libraries (scikit-learn, XGBoost, LightGBM).\n",
    "\n",
    "\n",
    "## Worked Numerical Example (Gini & Split)\n",
    "\n",
    "Small dataset (toy binary classification):\n",
    "\n",
    "| Sample |   x |  y |\n",
    "| -----: | --: | -: |\n",
    "|      1 | 2.0 |  0 |\n",
    "|      2 | 3.0 |  0 |\n",
    "|      3 | 4.0 |  1 |\n",
    "|      4 | 5.0 |  1 |\n",
    "\n",
    "Parent node: two 0s and two 1s → class probs $p_0=0.5, p_1=0.5$.\n",
    "\n",
    "Gini(parent) = $1 - (0.5^2 + 0.5^2) = 1 - 0.5 = 0.5$.\n",
    "\n",
    "Consider threshold between 3.0 and 4.0 → threshold = 3.5.\n",
    "Left child (x <= 3.5): samples 1 and 2 → labels \\[0,0] → Gini(left) = 0.0.\n",
    "Right child (x > 3.5): samples 3 and 4 → labels \\[1,1] → Gini(right) = 0.0.\n",
    "Weighted child Gini = 0.5*0 + 0.5*0 = 0.\n",
    "Gini decrease = 0.5 - 0 = 0.5 (perfect split).\n",
    "\n",
    "This split perfectly separates classes.\n",
    "\n",
    "---\n",
    "\n",
    "## Extensions & Production Notes\n",
    "\n",
    "* Support categorical features via one-hot or ordinal encoding.\n",
    "* Add sample weights for imbalanced datasets.\n",
    "* Implement MSE impurity for regression trees (predict mean at leaves).\n",
    "* Scale: for large datasets use efficient sorting/scanning or approximate histograms (LightGBM uses histogram-based splits).\n",
    "* For interpretability, produce textual rules from root-to-leaf paths.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "*End of document.*\n"
   ],
   "id": "5efd7d832a62ff53"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T20:22:33.852018Z",
     "start_time": "2025-08-10T20:22:33.841388Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from collections import Counter,namedtuple\n",
    "\n",
    "Node=namedtuple('Node',['feature','threshold','left','right','prediction','is_leaf'])\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T20:22:34.183087Z",
     "start_time": "2025-08-10T20:22:34.156182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from collections import Counter, namedtuple\n",
    "\n",
    "# Node structure\n",
    "Node = namedtuple(\"Node\", [\"feature\", \"threshold\", \"left\", \"right\", \"prediction\", \"is_leaf\"])\n",
    "\n",
    "class DecisionTreeClassifierFromScratch:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth if max_depth is not None else float(\"inf\")\n",
    "        self.min_samples_split = max(2, min_samples_split)\n",
    "        self.min_samples_leaf = max(1, min_samples_leaf)\n",
    "        self.root = None\n",
    "\n",
    "    # ---------- impurity measures ----------\n",
    "    @staticmethod\n",
    "    def gini(y):\n",
    "        if len(y) == 0:\n",
    "            return 0.0\n",
    "        counts = np.bincount(y)\n",
    "        ps = counts / counts.sum()\n",
    "        return 1.0 - np.sum(ps ** 2)\n",
    "\n",
    "    # ---------- utilities ----------\n",
    "    @staticmethod\n",
    "    def majority_class(y):\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "        counts = np.bincount(y)\n",
    "        return int(np.argmax(counts))\n",
    "\n",
    "    # ---------- best split ----------\n",
    "    def best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples < self.min_samples_split:\n",
    "            return None  # no split\n",
    "\n",
    "        parent_gini = self.gini(y)\n",
    "        best_gain = 0.0\n",
    "        best_feature, best_threshold = None, None\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            x_col = X[:, feature]\n",
    "            # consider only unique values\n",
    "            sorted_idx = np.argsort(x_col)\n",
    "            x_sorted = x_col[sorted_idx]\n",
    "            y_sorted = y[sorted_idx]\n",
    "\n",
    "            # potential thresholds are midpoints between consecutive distinct x where label changes\n",
    "            for i in range(1, n_samples):\n",
    "                if x_sorted[i] == x_sorted[i - 1]:\n",
    "                    continue  # same value, skip\n",
    "                # only consider threshold if label changes (saves computations)\n",
    "                if y_sorted[i] == y_sorted[i - 1]:\n",
    "                    # optionally still consider, but we can skip common values\n",
    "                    pass\n",
    "\n",
    "                thr = 0.5 * (x_sorted[i] + x_sorted[i - 1])\n",
    "\n",
    "                # split indices (since sorted, split at i)\n",
    "                y_left = y_sorted[:i]\n",
    "                y_right = y_sorted[i:]\n",
    "                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                gini_left = self.gini(y_left)\n",
    "                gini_right = self.gini(y_right)\n",
    "                w_left = len(y_left) / n_samples\n",
    "                w_right = len(y_right) / n_samples\n",
    "                gain = parent_gini - (w_left * gini_left + w_right * gini_right)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = thr\n",
    "\n",
    "        if best_feature is None:\n",
    "            return None\n",
    "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"gain\": best_gain}\n",
    "\n",
    "    # ---------- tree builder ----------\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        # create a leaf if stopping criteria met\n",
    "        num_samples = len(y)\n",
    "        pred = self.majority_class(y)\n",
    "        if num_samples == 0:\n",
    "            return Node(feature=None, threshold=None, left=None, right=None, prediction=None, is_leaf=True)\n",
    "        if (depth >= self.max_depth) or (num_samples < self.min_samples_split) or (self.gini(y) == 0.0):\n",
    "            return Node(feature=None, threshold=None, left=None, right=None, prediction=pred, is_leaf=True)\n",
    "\n",
    "        split = self.best_split(X, y)\n",
    "        if split is None:\n",
    "            return Node(feature=None, threshold=None, left=None, right=None, prediction=pred, is_leaf=True)\n",
    "\n",
    "        # perform split\n",
    "        f = split[\"feature\"]\n",
    "        t = split[\"threshold\"]\n",
    "        left_mask = X[:, f] <= t\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "            # degenerate split\n",
    "            return Node(feature=None, threshold=None, left=None, right=None, prediction=pred, is_leaf=True)\n",
    "\n",
    "        left_node = self.build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_node = self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return Node(feature=f, threshold=t, left=left_node, right=right_node, prediction=None, is_leaf=False)\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y).astype(int)\n",
    "        self.root = self.build_tree(X, y, depth=0)\n",
    "\n",
    "        # optional simple validation-based pruning: if a subtree replacement by a leaf improves\n",
    "        # validation accuracy, do it. We'll do a post-order traversal and try to prune.\n",
    "        if X_val is not None and y_val is not None:\n",
    "            self._prune_with_validation(self.root, X_val, y_val)\n",
    "\n",
    "    # ---------- prediction ----------\n",
    "    def _predict_one(self, x, node):\n",
    "        while not node.is_leaf:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.prediction if node.prediction is not None else 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "        preds = np.array([self._predict_one(row, self.root) for row in X])\n",
    "        return preds\n",
    "\n",
    "    # ---------- pruning ----------\n",
    "    def _prune_with_validation(self, node, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Post-order traversal: try replace node with leaf (majority class of node's training subset),\n",
    "        if validation accuracy doesn't decrease (or increases), keep the prune.\n",
    "        NOTE: For this to be correct, we would need to know node's training subset; for simplicity\n",
    "        we will approximate by computing predictions and testing pruning improvement globally.\n",
    "        \"\"\"\n",
    "        # helper to traverse and collect internal nodes\n",
    "        internal_nodes = []\n",
    "\n",
    "        def collect(n, path):\n",
    "            if n is None or n.is_leaf:\n",
    "                return\n",
    "            internal_nodes.append((n, path))\n",
    "            collect(n.left, path + \"L\")\n",
    "            collect(n.right, path + \"R\")\n",
    "        collect(node, \"\")\n",
    "\n",
    "        # baseline accuracy\n",
    "        base_preds = self.predict(X_val)\n",
    "        base_acc = np.mean(base_preds == y_val)\n",
    "\n",
    "        # try pruning each internal node: temporarily replace it with a leaf predicting majority class\n",
    "        # Because we don't have direct access to training samples per node, we guess prediction by majority\n",
    "        # of validation samples that fall in that node.\n",
    "        for n, path in internal_nodes:\n",
    "            # find validation samples reaching this node\n",
    "            masks = np.ones(len(X_val), dtype=bool)\n",
    "            # re-run path to get mask\n",
    "            cur = self.root\n",
    "            idxs = np.arange(len(X_val))\n",
    "            mask = np.ones(len(X_val), dtype=bool)\n",
    "            # follow the path string\n",
    "            for ch in path:\n",
    "                if cur.is_leaf:\n",
    "                    mask[:] = False; break\n",
    "                if ch == \"L\":\n",
    "                    mask = mask & (X_val[:, cur.feature] <= cur.threshold)\n",
    "                    cur = cur.left\n",
    "                else:\n",
    "                    mask = mask & (X_val[:, cur.feature] > cur.threshold)\n",
    "                    cur = cur.right\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            maj = Counter(y_val[mask]).most_common(1)[0][0]\n",
    "\n",
    "            # store original node\n",
    "            backup = Node(feature=n.feature, threshold=n.threshold, left=n.left, right=n.right, prediction=n.prediction, is_leaf=n.is_leaf)\n",
    "            # make it a leaf\n",
    "            n = n  # just alias\n",
    "            n_index = None  # we already have reference type; we must mutate node in place by setting attributes\n",
    "            # -> our Node is a namedtuple (immutable). For simplicity, we'll convert tree to mutable dict nodes in a fuller impl.\n",
    "            # To keep this simple and avoid heavy rewrite, we skip implementing destructive pruning here.\n",
    "            # Instead, we can mention this is a sketch: realistic pruning requires mutable nodes or storing training indices.\n",
    "            pass\n",
    "\n",
    "        # Because we used an immutable simple Node, full pruning implementation is longer.\n",
    "        # In production code, store training indices at each node to enable pruning exactly.\n",
    "\n",
    "    # ---------- utility evaluation ----------\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds == y)\n"
   ],
   "id": "176f78f1290cbd95",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T20:22:40.827094Z",
     "start_time": "2025-08-10T20:22:40.775187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # toy dataset: logical OR\n",
    "    X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "    y = np.array([0,1,1,1])\n",
    "    clf = DecisionTreeClassifierFromScratch(max_depth=2)\n",
    "    clf.fit(X, y)\n",
    "    print(\"preds:\", clf.predict(X))\n",
    "    print(\"accuracy:\", clf.score(X, y))\n"
   ],
   "id": "355f37bb5fd69656",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [0 1 1 1]\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
