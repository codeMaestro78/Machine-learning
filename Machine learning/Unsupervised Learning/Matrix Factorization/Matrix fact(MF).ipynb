{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9487cb0a",
   "metadata": {},
   "source": [
    "# Matrix Factorization in Unsupervised Learning\n",
    "\n",
    "## 1. Introduction\n",
    "Matrix factorization (MF) is a family of **unsupervised learning techniques** where a given data matrix is decomposed into a product of lower-dimensional matrices.\n",
    "\n",
    "- **Input**: A large matrix $$ X \\in \\mathbb{R}^{m \\times n} $$ (e.g., users vs. items, documents vs. words, pixels vs. images).  \n",
    "- **Goal**: Find matrices $$ W \\in \\mathbb{R}^{m \\times k} $$ and $$ H \\in \\mathbb{R}^{k \\times n} $$ with $$ k \\ll \\min(m,n) $$ such that:\n",
    "\n",
    "$$ X \\approx W H $$\n",
    "\n",
    "- **Output**: Latent factors (low-dimensional representations), useful for **clustering, compression, recommendation, denoising, feature extraction**, etc.\n",
    "\n",
    "It is *unsupervised* because we don’t have labels — the learning extracts hidden structures automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Core Concepts\n",
    "\n",
    "### 2.1 General Idea\n",
    "- **Matrix factorization = Dimensionality Reduction**  \n",
    "  Just like PCA, but often with additional constraints (non-negativity, sparsity, low-rank).  \n",
    "- The latent dimension $$ k $$ represents \"hidden features\" that explain the original matrix.\n",
    "\n",
    "### 2.2 Formal Setup\n",
    "Given:\n",
    "\n",
    "$$ X \\in \\mathbb{R}^{m \\times n}, \\quad X_{ij} = \\text{observed data (e.g., rating of user } i \\text{ on item } j\\text{)} $$\n",
    "\n",
    "We approximate:\n",
    "\n",
    "$$ X \\approx W H $$\n",
    "\n",
    "where:\n",
    "- $$ W \\in \\mathbb{R}^{m \\times k} $$: **latent representation of rows** (e.g., users, documents).  \n",
    "- $$ H \\in \\mathbb{R}^{k \\times n} $$: **latent representation of columns** (e.g., items, words).  \n",
    "- $$ k $$: rank/dimensionality of the latent space.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Types of Matrix Factorization in Unsupervised Learning\n",
    "\n",
    "### (a) Singular Value Decomposition (SVD) / PCA\n",
    "Factorization:\n",
    "\n",
    "$$ X = U \\Sigma V^T $$\n",
    "\n",
    "- $$ \\Sigma $$ gives singular values; $$ U, V $$ give orthogonal latent features.  \n",
    "- **Applications**: Dimensionality reduction, noise removal, latent semantic analysis (LSA in NLP).\n",
    "\n",
    "### (b) Non-negative Matrix Factorization (NMF)\n",
    "- Constraint: $$ W, H \\geq 0 $$ (all entries nonnegative).  \n",
    "- Leads to **parts-based, interpretable features**.  \n",
    "- **Applications**: Topic modeling, image decomposition, gene expression analysis.\n",
    "\n",
    "### (c) Probabilistic Matrix Factorization (PMF)\n",
    "- Treats factorization as a probabilistic model:\n",
    "\n",
    "$$ X_{ij} \\sim \\mathcal{N}(W_i H_j^T, \\sigma^2) $$\n",
    "\n",
    "- Basis of modern **collaborative filtering** in recommender systems.\n",
    "\n",
    "### (d) Sparse Matrix Factorization\n",
    "- Adds sparsity constraints on $$ W $$ or $$ H $$.  \n",
    "- Useful in high-dimensional settings (text, genomics).\n",
    "\n",
    "### (e) Tensor Factorization (extension)\n",
    "- Generalizes MF to multi-dimensional data (e.g., user × item × time).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Optimization Objective\n",
    "\n",
    "General form:\n",
    "\n",
    "$$ \\min_{W,H} \\; \\| X - W H \\|_F^2 + \\lambda (\\| W \\| + \\| H \\|) $$\n",
    "\n",
    "- First term: Reconstruction error (Frobenius norm).  \n",
    "- Second term: Regularization (to prevent overfitting).  \n",
    "- Constraints: Non-negativity, sparsity, orthogonality, etc., depending on method.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Applications\n",
    "\n",
    "- **Recommendation Systems**: Predict missing entries (Netflix Prize, Amazon).  \n",
    "- **Topic Modeling in NLP**: Documents-words matrix factorization → topics.  \n",
    "- **Computer Vision**: Image compression, facial recognition.  \n",
    "- **Bioinformatics**: Gene expression clustering, protein interactions.  \n",
    "- **Finance**: Extract latent factors driving stock returns.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Conceptual Intuition\n",
    "\n",
    "Matrix factorization can be seen as:\n",
    "- **Compression**: Turn high-dimensional data into a few interpretable latent features.  \n",
    "- **Discovery**: Extract hidden structures (topics, communities, parts).  \n",
    "- **Prediction**: Fill in missing data by learning low-dimensional patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb613c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      " [[1 1 0 0]\n",
      " [3 3 0 0]\n",
      " [0 0 4 4]\n",
      " [0 0 5 5]]\n",
      "W (documents in topic space):\n",
      " [[7.39195489e-16 5.61469919e-01]\n",
      " [0.00000000e+00 1.68440976e+00]\n",
      " [1.45406660e+00 0.00000000e+00]\n",
      " [1.81758325e+00 0.00000000e+00]]\n",
      "H (topics in word space):\n",
      " [[0.         0.         2.75090564 2.75090564]\n",
      " [1.78103931 1.78103931 0.         0.        ]]\n",
      "Reconstructed:\n",
      " [[1. 1. 0. 0.]\n",
      " [3. 3. 0. 0.]\n",
      " [0. 0. 4. 4.]\n",
      " [0. 0. 5. 5.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create a simple document-term matrix\n",
    "X = np.array([[1, 1, 0, 0],\n",
    "              [3, 3, 0, 0],\n",
    "              [0, 0, 4, 4],\n",
    "              [0, 0, 5, 5]])\n",
    "\n",
    "# Apply Non-negative Matrix Factorization\n",
    "nmf = NMF(n_components=2, init='random', random_state=0)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "print(\"Original Matrix:\\n\", X)\n",
    "print(\"W (documents in topic space):\\n\", W)\n",
    "print(\"H (topics in word space):\\n\", H)\n",
    "print(\"Reconstructed:\\n\", np.round(W @ H))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
